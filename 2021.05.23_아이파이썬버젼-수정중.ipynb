{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c24598e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Use multi-GPU setting ------\n",
      "if you stuck too long time with multi-GPU setting, try to set --workers 0\n",
      "--------------------------------------------------------------------------------\n",
      "dataset_root: /data/data/STARN/data_lmdb_release/training\n",
      "opt.select_data: ['ST']\n",
      "opt.batch_ratio: ['1']\n",
      "--------------------------------------------------------------------------------\n",
      "dataset_root:    /data/data/STARN/data_lmdb_release/training\t dataset: ST\n",
      "sub-directory:\t/ST\t num samples: 5522807\n",
      "sub-directory:\t/MJ/MJ_valid\t num samples: 802731\n",
      "sub-directory:\t/MJ/MJ_test\t num samples: 891924\n",
      "sub-directory:\t/MJ/MJ_train\t num samples: 7224586\n",
      "num total samples of ST: 14442048 x 1.0 (total_data_usage_ratio) = 14442048\n",
      "num samples of ST per batch: 3072 x 1.0 (batch_ratio) = 3072\n",
      "--------------------------------------------------------------------------------\n",
      "Total_batch_size: 3072 = 3072\n",
      "--------------------------------------------------------------------------------\n",
      "dataset_root:    /data/data/STARN/data_lmdb_release/validation\t dataset: /\n",
      "sub-directory:\t/.\t num samples: 6992\n",
      "model input parameters 32 100 20 1 512 256 38 25 TPS ResNet BiLSTM Attn\n",
      "Skip Transformation.LocalizationNetwork.localization_fc2.weight as it is already initialized\n",
      "Skip Transformation.LocalizationNetwork.localization_fc2.bias as it is already initialized\n",
      "torch.Size([768, 26, 256])\n",
      "torch.Size([768, 26, 256])\n",
      "torch.Size([768, 26, 256])\n",
      "torch.Size([768, 26, 256])\n",
      "torch.Size([768, 26, 256])\n",
      "torch.Size([768, 26, 256])\n",
      "torch.Size([768, 26, 256])\n",
      "torch.Size([768, 26, 256])\n",
      "torch.Size([768, 26, 256])torch.Size([768, 26, 256])\n",
      "\n",
      "torch.Size([768, 26, 256])torch.Size([768, 26, 256])\n",
      "\n",
      "torch.Size([212, 26, 256])\n",
      "torch.Size([212, 26, 256])\n",
      "torch.Size([212, 26, 256])\n",
      "torch.Size([212, 26, 256])\n",
      "[1/300000] Train loss: 3.67906, Valid loss: 3.61598, Elapsed_time: 19.83104\n",
      "Current_accuracy : 0.000, Current_norm_ED  : 0.01\n",
      "Best_accuracy    : 0.000, Best_norm_ED     : 0.01\n",
      "--------------------------------------------------------------------------------\n",
      "Ground Truth              | Prediction                | Confidence Score & T/F\n",
      "--------------------------------------------------------------------------------\n",
      "bus                       | 3ddgggggggggggggggggggggg | 0.0000\tFalse\n",
      "car                       | gddgggggggggggggggggggggg | 0.0000\tFalse\n",
      "roof                      | dsddddddgggnnxxggnnxxggnn | 0.0000\tFalse\n",
      "right                     | ggggggggggggggggggggggggg | 0.0000\tFalse\n",
      "mds                       | ggggggggggggggggggggggggg | 0.0000\tFalse\n",
      "--------------------------------------------------------------------------------\n",
      "torch.Size([768, 26, 256])\n",
      "torch.Size([768, 26, 256])\n",
      "torch.Size([768, 26, 256])\n",
      "torch.Size([768, 26, 256])\n",
      "torch.Size([768, 26, 256])\n",
      "torch.Size([768, 26, 256])torch.Size([768, 26, 256])\n",
      "\n",
      "torch.Size([768, 26, 256])\n",
      "torch.Size([768, 26, 256])torch.Size([768, 26, 256])torch.Size([768, 26, 256])\n",
      "\n",
      "\n",
      "torch.Size([768, 26, 256])\n",
      "torch.Size([212, 26, 256])\n",
      "torch.Size([212, 26, 256])\n",
      "torch.Size([212, 26, 256])\n",
      "torch.Size([212, 26, 256])\n",
      "[2/300000] Train loss: 3.41941, Valid loss: 3.40635, Elapsed_time: 37.37420\n",
      "Current_accuracy : 0.000, Current_norm_ED  : 0.00\n",
      "Best_accuracy    : 0.000, Best_norm_ED     : 0.01\n",
      "--------------------------------------------------------------------------------\n",
      "Ground Truth              | Prediction                | Confidence Score & T/F\n",
      "--------------------------------------------------------------------------------\n",
      "for                       |                           | 0.0000\tFalse\n",
      "india                     |                           | 0.0000\tFalse\n",
      "bos                       |                           | 0.0000\tFalse\n",
      "beware                    |                           | 0.0000\tFalse\n",
      "stadium                   |                           | 0.0000\tFalse\n",
      "--------------------------------------------------------------------------------\n",
      "torch.Size([768, 26, 256])\n",
      "torch.Size([768, 26, 256])\n",
      "torch.Size([768, 26, 256])\n",
      "torch.Size([768, 26, 256])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-23d563d3ce4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    330\u001b[0m         \"\"\"\n\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-23d563d3ce4b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(opt)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;31m## 그라디언트 https://sanghyu.tistory.com/87\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_clip\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# gradient clipping with 5 (Default)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mclip_coef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_norm\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtotal_norm\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mclip_coef\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip_coef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import numpy as np\n",
    "\n",
    "from utils import CTCLabelConverter, CTCLabelConverterForBaiduWarpctc, AttnLabelConverter, Averager\n",
    "from dataset import hierarchical_dataset, AlignCollate, Batch_Balanced_Dataset\n",
    "from model import Model\n",
    "from test import validation\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "\n",
    "import torch.onnx\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchviz import make_dot\n",
    "\n",
    "import easydict\n",
    "global opt\n",
    "opt = easydict.EasyDict({\n",
    "    \"exp_name\": \"test_01\",\n",
    "    \"train_data\": \"/data/data/STARN/data_lmdb_release/training\",\n",
    "    \"valid_data\":\"/data/data/STARN/data_lmdb_release/validation\",\n",
    "    \"manualSeed\": 1111,\n",
    "    \"workers\": 6,\n",
    "    \"batch_size\":768,\n",
    "    \"num_iter\":300000,\n",
    "    \"valInterval\":1,\n",
    "    \"saved_model\":'',\n",
    "    \"FT\":False,\n",
    "    \"adam\":False,\n",
    "    \"lr\":1,\n",
    "    \"beta1\":0.9,\n",
    "    \"rho\":0.95,\n",
    "    \"eps\":1e-8,\n",
    "    \"grad_clip\":5,\n",
    "    \"baiduCTC\":False,\n",
    "    \"select_data\":'ST',\n",
    "    \"batch_ratio\":'1',\n",
    "    \"total_data_usage_ratio\":'1.0',\n",
    "    \"batch_max_length\":25,\n",
    "    \"imgW\":100,\n",
    "    \"imgH\":32,\n",
    "    \"rgb\":False,\n",
    "    \"character\":\"0123456789abcdefghijklmnopqrstuvwxyz\",\n",
    "    \"sensitive\":False,\n",
    "    \"PAD\":False,\n",
    "    \"data_filtering_off\":False,\n",
    "    \"Transformation\":\"TPS\",\n",
    "    \"FeatureExtraction\":\"ResNet\",\n",
    "    \"SequenceModeling\":\"BiLSTM\",\n",
    "    \"Prediction\":'Attn',\n",
    "    \"num_fiducial\":20,\n",
    "    \"input_channel\":1,\n",
    "    \"output_channel\":512,\n",
    "    \"hidden_size\":256    \n",
    "})\n",
    "\n",
    "## batch_max_length 한 이미지에서의 최대 글자수 -> 디폴트값이 25이며, 한이미지에서 글자수 25자를 넘겨서는 안됨\n",
    "\n",
    "def train(opt):\n",
    "        \n",
    "        \n",
    "    opt.select_data = opt.select_data.split('-')\n",
    "    opt.batch_ratio = opt.batch_ratio.split('-')\n",
    "\n",
    "    train_dataset = Batch_Balanced_Dataset(opt)\n",
    "\n",
    "#     log = open(f'./saved_models/{opt.exp_name}/log_dataset.txt', 'a')\n",
    "    AlignCollate_valid = AlignCollate(imgH=opt.imgH, imgW=opt.imgW, keep_ratio_with_pad=opt.PAD)\n",
    "    valid_dataset, valid_dataset_log = hierarchical_dataset(root=opt.valid_data, opt=opt)\n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset, batch_size=opt.batch_size,\n",
    "        shuffle=True,  # 'True' to check training progress with validation function.\n",
    "        num_workers=int(opt.workers),\n",
    "        collate_fn=AlignCollate_valid, pin_memory=True)\n",
    "#     log.write(valid_dataset_log)\n",
    "#     print('-' * 80)\n",
    "#     log.write('-' * 80 + '\\n')\n",
    "#     log.close()\n",
    "    \n",
    "\n",
    "    converter = AttnLabelConverter(opt.character)\n",
    "    opt.num_class = len(converter.character)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    ## 인풋채널 3으로 하셈\n",
    "    opt.input_channel = 1\n",
    "    model = Model(opt)\n",
    "    print('model input parameters', opt.imgH, opt.imgW, opt.num_fiducial, opt.input_channel, opt.output_channel,\n",
    "          opt.hidden_size, opt.num_class, opt.batch_max_length, opt.Transformation, opt.FeatureExtraction,\n",
    "          opt.SequenceModeling, opt.Prediction)\n",
    "    \n",
    "    \n",
    "\n",
    "    ## 웨이트값 초기화\n",
    "    # weight initialization\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'localization_fc2' in name:\n",
    "            print(f'Skip {name} as it is already initialized')\n",
    "            continue\n",
    "        try:\n",
    "            if 'bias' in name:\n",
    "                init.constant_(param, 0.0)\n",
    "            elif 'weight' in name:\n",
    "                init.kaiming_normal_(param)\n",
    "        except Exception as e:  # for batchnorm.\n",
    "            if 'weight' in name:\n",
    "                param.data.fill_(1)\n",
    "            continue\n",
    "\n",
    "            \n",
    "            \n",
    "    # data parallel for multi-GPU\n",
    "    model = torch.nn.DataParallel(model).to(device)\n",
    "    model.train()\n",
    "    \n",
    "    #\n",
    "    if opt.saved_model != '':\n",
    "        print(f'loading pretrained model from {opt.saved_model}')\n",
    "        if opt.FT:\n",
    "            model.load_state_dict(torch.load(opt.saved_model), strict=False)\n",
    "        else:\n",
    "            model.load_state_dict(torch.load(opt.saved_model))\n",
    "#     print(\"Model:\")\n",
    "#     print(model)\n",
    "\n",
    "#     \"\"\" setup loss \"\"\"\n",
    "#     if 'CTC' in opt.Prediction:\n",
    "#         if opt.baiduCTC:\n",
    "#             # need to install warpctc. see our guideline.\n",
    "#             from warpctc_pytorch import CTCLoss \n",
    "#             criterion = CTCLoss()\n",
    "#         else:\n",
    "#             criterion = torch.nn.CTCLoss(zero_infinity=True).to(device)\n",
    "#     else:\n",
    "\n",
    "    ## 로스값 구할때 go 토큰은 제외하고 계산\n",
    "    criterion = torch.nn.CrossEntropyLoss(ignore_index=0).to(device)  # ignore [GO] token = ignore index 0\n",
    "    # loss averager\n",
    "    loss_avg = Averager()\n",
    "\n",
    "    \n",
    "    \n",
    "    ## 이부분은 모르겟음\n",
    "    ## 단순 학습가능한 파라메터를 출력 해주기 위함\n",
    "#     filter that only require gradient decent\n",
    "    filtered_parameters = []\n",
    "    params_num = []\n",
    "    for p in filter(lambda p: p.requires_grad, model.parameters()):\n",
    "        filtered_parameters.append(p)\n",
    "        params_num.append(np.prod(p.size()))\n",
    "#     print('Trainable params num : ', sum(params_num))\n",
    "    # [print(name, p.numel()) for name, p in filter(lambda p: p[1].requires_grad, model.named_parameters())]\n",
    "\n",
    "    \n",
    "    ## 옵티마이져 셋팅\n",
    "    # setup optimizer\n",
    "    if opt.adam:\n",
    "        optimizer = optim.Adam(filtered_parameters, lr=opt.lr, betas=(opt.beta1, 0.999))\n",
    "    else:\n",
    "        optimizer = optim.Adadelta(filtered_parameters, lr=opt.lr, rho=opt.rho, eps=opt.eps)\n",
    "#     print(\"Optimizer:\")\n",
    "#     print(optimizer)\n",
    "\n",
    "    \n",
    "    \n",
    "#     ## 옵션 찍는거 삭제해도 됨\n",
    "#     \"\"\" final options \"\"\"\n",
    "#     # print(opt)\n",
    "#     with open(f'./saved_models/{opt.exp_name}/opt.txt', 'a') as opt_file:\n",
    "#         opt_log = '------------ Options -------------\\n'\n",
    "#         args = vars(opt)\n",
    "#         for k, v in args.items():\n",
    "#             opt_log += f'{str(k)}: {str(v)}\\n'\n",
    "#         opt_log += '---------------------------------------\\n'\n",
    "#         print(opt_log)\n",
    "#         opt_file.write(opt_log)\n",
    "\n",
    "#     \"\"\" start training \"\"\"\n",
    "    start_iter = 0\n",
    "#     if opt.saved_model != '':\n",
    "#         try:\n",
    "#             start_iter = int(opt.saved_model.split('_')[-1].split('.')[0])\n",
    "#             print(f'continue to train, start_iter: {start_iter}')\n",
    "#         except:\n",
    "#             pass\n",
    "\n",
    "        \n",
    "        \n",
    "    start_time = time.time()\n",
    "    best_accuracy = -1\n",
    "    best_norm_ED = -1\n",
    "    iteration = start_iter\n",
    "\n",
    "    while(True):\n",
    "        # train part\n",
    "        image_tensors, labels = train_dataset.get_batch()\n",
    "        image = image_tensors.to(device)\n",
    "        text, length = converter.encode(labels, batch_max_length=opt.batch_max_length)\n",
    "        batch_size = image.size(0)\n",
    "\n",
    "        if 'CTC' in opt.Prediction:\n",
    "            preds = model(image, text)\n",
    "            preds_size = torch.IntTensor([preds.size(1)] * batch_size)\n",
    "            if opt.baiduCTC:\n",
    "                preds = preds.permute(1, 0, 2)  # to use CTCLoss format\n",
    "                cost = criterion(preds, text, preds_size, length) / batch_size\n",
    "            else:\n",
    "                preds = preds.log_softmax(2).permute(1, 0, 2)\n",
    "                cost = criterion(preds, text, preds_size, length)\n",
    "\n",
    "        else:\n",
    "            preds = model(image, text[:, :-1])  # align with Attention.forward\n",
    "            target = text[:, 1:]  # without [GO] Symbol\n",
    "            cost = criterion(preds.view(-1, preds.shape[-1]), target.contiguous().view(-1))\n",
    "\n",
    "        model.zero_grad()\n",
    "        cost.backward()\n",
    "        \n",
    "        ## 그라디언트 https://sanghyu.tistory.com/87\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), opt.grad_clip)  # gradient clipping with 5 (Default)\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_avg.add(cost)\n",
    "\n",
    "        # validation part\n",
    "        if (iteration + 1) % opt.valInterval == 0 or iteration == 0: # To see training progress, we also conduct validation when 'iteration == 0' \n",
    "            elapsed_time = time.time() - start_time\n",
    "            # for log\n",
    "            with open(f'./saved_models/{opt.exp_name}/log_train.txt', 'a') as log:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    valid_loss, current_accuracy, current_norm_ED, preds, confidence_score, labels, infer_time, length_of_data = validation(\n",
    "                        model, criterion, valid_loader, converter, opt)\n",
    "                model.train()\n",
    "\n",
    "                # training loss and validation loss\n",
    "                loss_log = f'[{iteration+1}/{opt.num_iter}] Train loss: {loss_avg.val():0.5f}, Valid loss: {valid_loss:0.5f}, Elapsed_time: {elapsed_time:0.5f}'\n",
    "                loss_avg.reset()\n",
    "\n",
    "                current_model_log = f'{\"Current_accuracy\":17s}: {current_accuracy:0.3f}, {\"Current_norm_ED\":17s}: {current_norm_ED:0.2f}'\n",
    "\n",
    "                # keep best accuracy model (on valid dataset)\n",
    "                if current_accuracy > best_accuracy:\n",
    "                    best_accuracy = current_accuracy\n",
    "                    torch.save(model.state_dict(), f'./saved_models/{opt.exp_name}/best_accuracy.pth')\n",
    "                if current_norm_ED > best_norm_ED:\n",
    "                    best_norm_ED = current_norm_ED\n",
    "                    torch.save(model.state_dict(), f'./saved_models/{opt.exp_name}/best_norm_ED.pth')\n",
    "                best_model_log = f'{\"Best_accuracy\":17s}: {best_accuracy:0.3f}, {\"Best_norm_ED\":17s}: {best_norm_ED:0.2f}'\n",
    "\n",
    "                loss_model_log = f'{loss_log}\\n{current_model_log}\\n{best_model_log}'\n",
    "                print(loss_model_log)\n",
    "                log.write(loss_model_log + '\\n')\n",
    "\n",
    "                # show some predicted results\n",
    "                dashed_line = '-' * 80\n",
    "                head = f'{\"Ground Truth\":25s} | {\"Prediction\":25s} | Confidence Score & T/F'\n",
    "                predicted_result_log = f'{dashed_line}\\n{head}\\n{dashed_line}\\n'\n",
    "                for gt, pred, confidence in zip(labels[:5], preds[:5], confidence_score[:5]):\n",
    "                    if 'Attn' in opt.Prediction:\n",
    "                        gt = gt[:gt.find('[s]')]\n",
    "                        pred = pred[:pred.find('[s]')]\n",
    "\n",
    "                    predicted_result_log += f'{gt:25s} | {pred:25s} | {confidence:0.4f}\\t{str(pred == gt)}\\n'\n",
    "                predicted_result_log += f'{dashed_line}'\n",
    "                print(predicted_result_log)\n",
    "                log.write(predicted_result_log + '\\n')\n",
    "\n",
    "        # save model per 1e+5 iter.\n",
    "        if (iteration + 1) % 1e+5 == 0:\n",
    "            torch.save(\n",
    "                model.state_dict(), f'./saved_models/{opt.exp_name}/iter_{iteration+1}.pth')\n",
    "\n",
    "        if (iteration + 1) == opt.num_iter:\n",
    "            print('end the training')\n",
    "            sys.exit()\n",
    "        iteration += 1\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    if not opt.exp_name:\n",
    "        opt.exp_name = f'{opt.Transformation}-{opt.FeatureExtraction}-{opt.SequenceModeling}-{opt.Prediction}'\n",
    "        opt.exp_name += f'-Seed{opt.manualSeed}'\n",
    "        # print(opt.exp_name)\n",
    "\n",
    "    os.makedirs(f'./saved_models/{opt.exp_name}', exist_ok=True)\n",
    "\n",
    "    \"\"\" vocab / character number configuration \"\"\"\n",
    "    if opt.sensitive:\n",
    "        # opt.character += 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "        opt.character = string.printable[:-6]  # same with ASTER setting (use 94 char).\n",
    "\n",
    "    \"\"\" Seed and GPU setting \"\"\"\n",
    "    # print(\"Random Seed: \", opt.manualSeed)\n",
    "    random.seed(opt.manualSeed)\n",
    "    np.random.seed(opt.manualSeed)\n",
    "    torch.manual_seed(opt.manualSeed)\n",
    "    torch.cuda.manual_seed(opt.manualSeed)\n",
    "\n",
    "    cudnn.benchmark = True\n",
    "    cudnn.deterministic = True\n",
    "    opt.num_gpu = torch.cuda.device_count()\n",
    "    # print('device count', opt.num_gpu)\n",
    "    if opt.num_gpu > 1:\n",
    "        print('------ Use multi-GPU setting ------')\n",
    "        print('if you stuck too long time with multi-GPU setting, try to set --workers 0')\n",
    "        # check multi-GPU issue https://github.com/clovaai/deep-text-recognition-benchmark/issues/1\n",
    "        opt.workers = opt.workers * opt.num_gpu\n",
    "        opt.batch_size = opt.batch_size * opt.num_gpu\n",
    "\n",
    "        \"\"\" previous version\n",
    "        print('To equlize batch stats to 1-GPU setting, the batch_size is multiplied with num_gpu and multiplied batch_size is ', opt.batch_size)\n",
    "        opt.batch_size = opt.batch_size * opt.num_gpu\n",
    "        print('To equalize the number of epochs to 1-GPU setting, num_iter is divided with num_gpu by default.')\n",
    "        If you dont care about it, just commnet out these line.)\n",
    "        opt.num_iter = int(opt.num_iter / opt.num_gpu)\n",
    "        \"\"\"\n",
    "\n",
    "    train(opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe1c70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import CTCLabelConverter, CTCLabelConverterForBaiduWarpctc, AttnLabelConverter, Averager\n",
    "\n",
    "converter = AttnLabelConverter(\"0123456789abcdefghijklmnopqrstuvwxyz\")\n",
    "# print(converter.character)\n",
    "print(converter.dict)\n",
    "\n",
    "temp_text = [\"zxxxxxcv\",\"abwwqwecd\"]\n",
    "\n",
    "print(converter.encode(temp_text))\n",
    "num_class = len(converter.character)\n",
    "print(num_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db8a4dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
