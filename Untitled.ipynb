{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f7ccfee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.4687],\n",
      "          [-1.0357],\n",
      "          [ 0.3042]],\n",
      "\n",
      "         [[-0.7576],\n",
      "          [-0.8016],\n",
      "          [ 1.9441]]],\n",
      "\n",
      "\n",
      "        [[[-1.0119],\n",
      "          [-0.7499],\n",
      "          [ 0.2904]],\n",
      "\n",
      "         [[ 0.3945],\n",
      "          [-0.0525],\n",
      "          [ 0.4368]]]])\n",
      "torch.Size([2, 2, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import six\n",
    "import math\n",
    "import lmdb\n",
    "import torch\n",
    "\n",
    "from natsort import natsorted\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, ConcatDataset, Subset\n",
    "from torch._utils import _accumulate\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "\n",
    "\n",
    "ft = torch.Tensor(torch.randn(2, 2, 3, 1))\n",
    "print(ft)\n",
    "print(ft.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "903d3f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.4687, -1.0357,  0.3042],\n",
      "         [-0.7576, -0.8016,  1.9441]],\n",
      "\n",
      "        [[-1.0119, -0.7499,  0.2904],\n",
      "         [ 0.3945, -0.0525,  0.4368]]])\n",
      "torch.Size([2, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "print(ft.squeeze(3))\n",
    "print(ft.squeeze(3).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e3b5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attention_cell = AttentionCell(input_size, hidden_size, num_classes)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_classes = num_classes\n",
    "        self.generator = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def _char_to_onehot(self, input_char, onehot_dim=38):\n",
    "        input_char = input_char.unsqueeze(1)\n",
    "        batch_size = input_char.size(0)\n",
    "        one_hot = torch.FloatTensor(batch_size, onehot_dim).zero_().to(device)\n",
    "        one_hot = one_hot.scatter_(1, input_char, 1)\n",
    "        return one_hot\n",
    "\n",
    "    def forward(self, batch_H, text, is_train=True, batch_max_length=25):\n",
    "        \"\"\"\n",
    "        input:\n",
    "            batch_H : contextual_feature H = hidden state of encoder. [batch_size x num_steps x contextual_feature_channels]\n",
    "            text : the text-index of each image. [batch_size x (max_length+1)]. +1 for [GO] token. text[:, 0] = [GO].\n",
    "        output: probability distribution at each step [batch_size x num_steps x num_classes]\n",
    "        \"\"\"\n",
    "        \"\" \"\n",
    "        # 입력:\n",
    "        #      batch_H : contextual_feature H = 인코더의 숨겨진 상태. [batch_size x num_steps x contextual_feature_channels]\n",
    "        #      text : 각 이미지의 텍스트 인덱스. [배치 _ 크기 x (최대 _ 길이 +1)]. [GO] 토큰에 +1. 텍스트 [:, 0] = [GO].\n",
    "        # 출력 : 각 단계의 확률 분포 [batch_size x num_steps x num_classes]\n",
    "        # \"\" \"\n",
    "        \n",
    "        batch_size = batch_H.size(0)\n",
    "        \n",
    "        ## num_steps 26번 해야한다!\n",
    "        num_steps = batch_max_length + 1  # +1 for [s] at end of sentence.\n",
    "\n",
    "        output_hiddens = torch.FloatTensor(batch_size, num_steps, self.hidden_size).fill_(0).to(device)\n",
    "        hidden = (torch.FloatTensor(batch_size, self.hidden_size).fill_(0).to(device),\n",
    "                  torch.FloatTensor(batch_size, self.hidden_size).fill_(0).to(device))\n",
    "\n",
    "        if is_train:\n",
    "            for i in range(num_steps):\n",
    "                # one-hot vectors for a i-th char. in a batch\n",
    "                char_onehots = self._char_to_onehot(text[:, i], onehot_dim=self.num_classes)\n",
    "                # hidden : decoder's hidden s_{t-1}, batch_H : encoder's hidden H, char_onehots : one-hot(y_{t-1})\n",
    "                hidden, alpha = self.attention_cell(hidden, batch_H, char_onehots)\n",
    "                output_hiddens[:, i, :] = hidden[0]  # LSTM hidden index (0: hidden, 1: Cell)\n",
    "            probs = self.generator(output_hiddens)\n",
    "\n",
    "        else:\n",
    "            targets = torch.LongTensor(batch_size).fill_(0).to(device)  # [GO] token\n",
    "            probs = torch.FloatTensor(batch_size, num_steps, self.num_classes).fill_(0).to(device)\n",
    "\n",
    "            for i in range(num_steps):\n",
    "                char_onehots = self._char_to_onehot(targets, onehot_dim=self.num_classes)\n",
    "                hidden, alpha = self.attention_cell(hidden, batch_H, char_onehots)\n",
    "                probs_step = self.generator(hidden[0])\n",
    "                probs[:, i, :] = probs_step\n",
    "                _, next_input = probs_step.max(1)\n",
    "                targets = next_input\n",
    "        \n",
    "        # probs는 3차원 확확률값이다.\n",
    "        # torch.Size([768, 26, 38])\n",
    "        return probs  # batch_size x num_steps x num_classes\n",
    "\n",
    "\n",
    "class AttentionCell(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_embeddings):\n",
    "        super(AttentionCell, self).__init__()\n",
    "        self.i2h = nn.Linear(input_size, hidden_size, bias=False)\n",
    "        self.h2h = nn.Linear(hidden_size, hidden_size)  # either i2i or h2h should have bias\n",
    "        self.score = nn.Linear(hidden_size, 1, bias=False)\n",
    "        self.rnn = nn.LSTMCell(input_size + num_embeddings, hidden_size)\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def forward(self, prev_hidden, batch_H, char_onehots):\n",
    "        # [batch_size x num_encoder_step x num_channel] -> [batch_size x num_encoder_step x hidden_size]\n",
    "        batch_H_proj = self.i2h(batch_H)\n",
    "        prev_hidden_proj = self.h2h(prev_hidden[0]).unsqueeze(1)\n",
    "        e = self.score(torch.tanh(batch_H_proj + prev_hidden_proj))  # batch_size x num_encoder_step * 1\n",
    "\n",
    "        alpha = F.softmax(e, dim=1)\n",
    "        context = torch.bmm(alpha.permute(0, 2, 1), batch_H).squeeze(1)  # batch_size x num_channel\n",
    "        concat_context = torch.cat([context, char_onehots], 1)  # batch_size x (num_channel + num_embedding)\n",
    "        cur_hidden = self.rnn(concat_context, prev_hidden)\n",
    "        return cur_hidden, alpha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb861041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768, 26, 38])\n"
     ]
    }
   ],
   "source": [
    "probs = torch.FloatTensor(768, 26, 38)\n",
    "print(probs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10f2773",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
